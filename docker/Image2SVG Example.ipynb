{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659c25fd-2a10-4010-ae4c-bf6c42c59a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 01:54:37,707] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e87479fcbb4f9ba8ed1289bafe288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 7507080192 || trainable%: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379283532e2e4c14b647f9908332ab52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StarVectorForCausalLM(\n",
       "  (model): StarVectorStarCoder2(\n",
       "    (svg_transformer): StarCoderModel(\n",
       "      (transformer): Starcoder2ForCausalLM(\n",
       "        (model): Starcoder2Model(\n",
       "          (embed_tokens): Embedding(49157, 4608)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x Starcoder2DecoderLayer(\n",
       "              (self_attn): Starcoder2Attention(\n",
       "                (q_proj): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "                (k_proj): Linear(in_features=4608, out_features=512, bias=True)\n",
       "                (v_proj): Linear(in_features=4608, out_features=512, bias=True)\n",
       "                (o_proj): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "              )\n",
       "              (mlp): Starcoder2MLP(\n",
       "                (c_fc): Linear(in_features=4608, out_features=18432, bias=True)\n",
       "                (c_proj): Linear(in_features=18432, out_features=4608, bias=True)\n",
       "                (act): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "              (post_attention_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary_emb): Starcoder2RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4608, out_features=49157, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (image_encoder): ImageEncoder(\n",
       "      (visual_encoder): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "          (position_embedding): Embedding(576, 1024)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x SiglipEncoderLayer(\n",
       "              (self_attn): SiglipSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (head): SiglipMultiheadAttentionPoolingHead(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (image_projection): Adapter(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (c_fc): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (act): Swish()\n",
       "      (c_proj): Linear(in_features=2048, out_features=4608, bias=True)\n",
       "      (norm): LayerNorm((576, 4608), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from starvector.model.starvector_arch import StarVectorForCausalLM\n",
    "from starvector.data.util import process_and_rasterize_svg\n",
    "import torch\n",
    "\n",
    "#model_name = \"starvector/starvector-1b-im2svg\"\n",
    "model_name = \"starvector/starvector-8b-im2svg\"\n",
    "\n",
    "#starvector = StarVectorForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "starvector = StarVectorForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "starvector.cuda()\n",
    "starvector.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e226406-f354-46e7-8962-dbdd4b150789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "image_pil = Image.open('assets/examples/sample-0.png')\n",
    "image = starvector.process_images([image_pil])[0].cuda()\n",
    "if not image.shape[0] == 1:\n",
    "    image = image.squeeze(0)\n",
    "batch = {\"image\": image}\n",
    "\n",
    "raw_svg = starvector.generate_im2svg(batch, max_length=1000000)[0]\n",
    "svg, raster_image = process_and_rasterize_svg(raw_svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b9e860-a969-4397-bca1-8f21d3a0d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "raw_svg = starvector.generate_im2svg(batch, max_length=1000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33d29e80-1d9e-4fa5-bf7e-ae91afb15171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 1000 1000\">\n",
      "  <defs>\n",
      "    <style>\n",
      "     .cls-1 {\n",
      "        fill: none;\n",
      "      }\n",
      "\n",
      "     .cls-2 {\n",
      "        clip-path: url(#clip-path);\n",
      "      }\n",
      "\n",
      "     .cls-3 {\n",
      "        fill: #252c40;\n",
      "      }\n",
      "\n",
      "     .cls-4 {\n",
      "        fill: #fff;\n",
      "      }\n",
      "\n",
      "     .cls-5 {\n",
      "        fill: #272d41;\n",
      "      }\n",
      "\n",
      "     .cls-6 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-7 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-8 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-9 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-10 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-11 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-12 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-13 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-14 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-15 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-16 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-17 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-18 {\n",
      "        fill: #262d41;\n",
      "      }\n",
      "\n",
      "     .cls-19 {\n",
      "        fill: #262d41;\n",
      "      }\n"
     ]
    }
   ],
   "source": [
    "print(raw_svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119bcb88-e5e2-499e-bb22-942b30115f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVG内容长度: 647\n",
      "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 1010 250\"><defs><style>.cls-1{fill:none;}.cls-2{clip-path:url(#clip-path);}.cls-3{fill:#252c40;}</style>\n"
     ]
    }
   ],
   "source": [
    "# 在生成SVG后，打印其内容以检查是否真的为空  \n",
    "print(f\"SVG内容长度: {len(raw_svg)}\")  \n",
    "print(raw_svg[:200])  # 打印SVG开头部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "409daf37-680d-4c99-8ef8-00dc4a64e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg></svg>\n"
     ]
    }
   ],
   "source": [
    "print(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccfbc6b-4d25-4563-a882-306c6a05cabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "display(image_pil)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a09a6b6-a68d-4d75-b2b2-581dafd15c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg/>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAE7UlEQVR4Ae3TARUAIAyEULV/5+2Zg78GcOPOzHEMVA28KjhuBr4BAfiDtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wQvAD6QNCCA9P3gB+IG0AQGk5wcvAD+QNiCA9PzgBeAH0gYEkJ4fvAD8QNqAANLzgxeAH0gbEEB6fvAC8ANpAwJIzw9eAH4gbUAA6fnBC8APpA0IID0/eAH4gbQBAaTnBy8AP5A2IID0/OAF4AfSBgSQnh+8APxA2oAA0vODF4AfSBsQQHp+8ALwA2kDAkjPD14AfiBtQADp+cELwA+kDQggPT94AfiBtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wQvAD6QNCCA9P3gB+IG0AQGk5wcvAD+QNiCA9PzgBeAH0gYEkJ4fvAD8QNqAANLzgxeAH0gbEEB6fvAC8ANpAwJIzw9eAH4gbUAA6fnBC8APpA0IID0/eAH4gbQBAaTnBy8AP5A2IID0/OAF4AfSBgSQnh+8APxA2oAA0vODF4AfSBsQQHp+8ALwA2kDAkjPD14AfiBtQADp+cELwA+kDQggPT94AfiBtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wQvAD6QNCCA9P3gB+IG0AQGk5wcvAD+QNiCA9PzgBeAH0gYEkJ4fvAD8QNqAANLzgxeAH0gbEEB6fvAC8ANpAwJIzw9eAH4gbUAA6fnBC8APpA0IID0/eAH4gbQBAaTnBy8AP5A2IID0/OAF4AfSBgSQnh+8APxA2oAA0vODF4AfSBsQQHp+8ALwA2kDAkjPD14AfiBtQADp+cELwA+kDQggPT94AfiBtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wQvAD6QNCCA9P3gB+IG0AQGk5wcvAD+QNiCA9PzgBeAH0gYEkJ4fvAD8QNqAANLzgxeAH0gbEEB6fvAC8ANpAwJIzw9eAH4gbUAA6fnBC8APpA0IID0/eAH4gbQBAaTnBy8AP5A2IID0/OAF4AfSBgSQnh+8APxA2oAA0vODF4AfSBsQQHp+8ALwA2kDAkjPD14AfiBtQADp+cELwA+kDQggPT94AfiBtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wQvAD6QNCCA9P3gB+IG0AQGk5wcvAD+QNiCA9PzgBeAH0gYEkJ4fvAD8QNqAANLzgxeAH0gbEEB6fvAC8ANpAwJIzw9eAH4gbUAA6fnBC8APpA0IID0/eAH4gbQBAaTnBy8AP5A2IID0/OAF4AfSBgSQnh+8APxA2oAA0vODF4AfSBsQQHp+8ALwA2kDAkjPD14AfiBtQADp+cELwA+kDQggPT94AfiBtAEBpOcHLwA/kDYggPT84AXgB9IGBJCeH7wA/EDagADS84MXgB9IGxBAen7wAvADaQMCSM8PXgB+IG1AAOn5wS+r6AT9kjYVNgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(SVG(svg))\n",
    "display(raster_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3ae9d8-7e7c-4eba-b830-4f45d68f7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像模式: RGBA, 图像大小: (1705, 855)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m image = starvector.process_images([image_pil])[\u001b[32m0\u001b[39m].cuda()\n\u001b[32m      8\u001b[39m batch = {\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: image}\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m raw_svg = \u001b[43mstarvector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_im2svg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m svg, raster_image = process_and_rasterize_svg(raw_svg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/starvector/model/starvector_arch.py:187\u001b[39m, in \u001b[36mStarVectorForCausalLM.generate_im2svg\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_im2svg\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_im2svg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/starvector/model/models/starvector_base.py:255\u001b[39m, in \u001b[36mStarVectorBase.generate_im2svg\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Let subclasses override these defaults if needed\u001b[39;00m\n\u001b[32m    253\u001b[39m generation_kwargs.update(\u001b[38;5;28mself\u001b[39m._get_im2svg_specific_kwargs(kwargs))\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msvg_transformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m outputs = torch.cat([prompt_tokens.input_ids, outputs], dim=\u001b[32m1\u001b[39m)\n\u001b[32m    257\u001b[39m raw_svg = \u001b[38;5;28mself\u001b[39m.svg_transformer.tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/generation/utils.py:2254\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2246\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2247\u001b[39m         input_ids=input_ids,\n\u001b[32m   2248\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2249\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2250\u001b[39m         **model_kwargs,\n\u001b[32m   2251\u001b[39m     )\n\u001b[32m   2253\u001b[39m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2254\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2255\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2264\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2265\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2266\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2267\u001b[39m         batch_size=batch_size,\n\u001b[32m   2268\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2274\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2275\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/generation/utils.py:3463\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3460\u001b[39m     outputs = stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m.config.get_text_config())\n\u001b[32m   3462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3463\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3466\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3467\u001b[39m     outputs,\n\u001b[32m   3468\u001b[39m     model_kwargs,\n\u001b[32m   3469\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3470\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:839\u001b[39m, in \u001b[36mStarcoder2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:562\u001b[39m, in \u001b[36mStarcoder2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    560\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:252\u001b[39m, in \u001b[36mStarcoder2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:188\u001b[39m, in \u001b[36mStarcoder2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    187\u001b[39m cos, sin = position_embeddings\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m query_states, key_states = \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    192\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:111\u001b[39m, in \u001b[36mapply_rotary_pos_emb\u001b[39m\u001b[34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[39m\n\u001b[32m    109\u001b[39m sin = sin.unsqueeze(unsqueeze_dim)\n\u001b[32m    110\u001b[39m q_embed = (q * cos) + (rotate_half(q) * sin)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m k_embed = (k * cos) + (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m * sin)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/starvector/lib/python3.11/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:81\u001b[39m, in \u001b[36mrotate_half\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     77\u001b[39m         hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.residual_dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrotate_half\u001b[39m(x):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m     x1 = x[..., : x.shape[-\u001b[32m1\u001b[39m] // \u001b[32m2\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#image_pil = Image.open('../assets/examples/sample-18.png')\n",
    "# 检查图像是否正确加载  \n",
    "image_pil = Image.open('./pic2.png')  \n",
    "print(f\"图像模式: {image_pil.mode}, 图像大小: {image_pil.size}\")  \n",
    "# 转换为RGB并继续处理  \n",
    "image_pil = image_pil.convert('RGB')  \n",
    "image = starvector.process_images([image_pil])[0].cuda()\n",
    "batch = {\"image\": image}\n",
    "\n",
    "raw_svg = starvector.generate_im2svg(batch, max_length=10000)[0]\n",
    "svg, raster_image = process_and_rasterize_svg(raw_svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a100757-c512-4e4b-a05d-f5a74fbf9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "display(image_pil)\n",
    "display(SVG(svg))\n",
    "display(raster_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d862e-808c-4086-9366-b369f5a24427",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61a1ee-f2f9-46f7-8881-9597e5c50559",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(svg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb02e06-b36c-4be2-aaba-de900988bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raster_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b13dd1af-d579-452a-96f8-f0fae5289d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "del starvector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1d4d9-2798-4b67-9942-7b3a5cf03122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
